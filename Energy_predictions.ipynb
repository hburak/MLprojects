{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f19116",
   "metadata": {},
   "source": [
    "# Exploratory data analysis,Anomaly Detection and ML predictions on Time Series energy and power datasests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printed tables and result can't be shown on github since the all file is too large to render by Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8828f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Importing neccesary libs as initial\n",
    "#İlk gerekli kütüphanelerin yüklenmesi \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "features =pd.read_csv (\"features.csv\", sep=',')\n",
    "df = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial xploratory data analysis steps\n",
    "#Keşifçi ön analiz işlemleri\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see  data types\n",
    "#Veri türlerini görelim\n",
    "df.info ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see descriptive statistics\n",
    "## Betimsel istatistikleri alalım\n",
    "df.describe (include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b50dbc",
   "metadata": {},
   "source": [
    "#### Date time variables should be converted to DateTime format in order to let run ML algorithms properly\n",
    "\n",
    "#### \"DateTime\" değişkeninin ML algoritması için uygun tarih formatına çevrilmesi ve Object yerine DateTime formatında kaydedilmesi gerekir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] =  pd.to_datetime(df['Timestamp'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gaps in column title names have been filled properly \n",
    "#### Sütun başlıklarındaki boşluklar dolduruldu\n",
    "df.columns = df.columns.str.replace(' ','_')\n",
    "df.columns = df.columns.str.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have imported power.csv as well , we gona merge it later\n",
    "#Power veri setini çektik\n",
    "power=pd.read_csv (\"power.csv\", sep=',')\n",
    "power.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a56407",
   "metadata": {},
   "outputs": [],
   "source": [
    "power.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by using df.describe() method, \"-273\" values for first three numeric columns have been placed as minimum values.\n",
    "# so it is better to call min values for all columns in order to see if there is sytematic anomaly for all columns \n",
    "\n",
    "# df. describe() ile sütunlarda min değerlere baktığımızda -273'lerin ilk 3 nümerik değişkende min değer olduğunu gördük\n",
    "#burdan hareketle daha sonra lazım olabilir diye min value değerleri ayrıyetten çekmek istedik\n",
    "minvalues = df.min()\n",
    "minvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe2414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, IQR statistical method have been implemented to detect  all anormalies for all columns. And all printed values have been saved as series.\n",
    "#  Veri seti içindeki tüm nümerik değişkenlerin hepsini gezerek anomali tespiti yapıp outlier olarak kaydedecek formülü yazdık. Ve series kaydettik\n",
    "\n",
    "def find_outliers(col):\n",
    "    q1 = col.quantile(.25)\n",
    "    q3 = col.quantile(.75)\n",
    "    IQR = q3 - q1\n",
    "    ll = q1 - (1.5*IQR)\n",
    "    ul = q3 + (1.5*IQR)\n",
    "    upper_outliers = col[col > ul].tolist()\n",
    "    lower_outliers = col[col < ll].tolist()\n",
    "    bad_indices = list(set(upper_outliers + lower_outliers))\n",
    "    return(bad_indices)\n",
    "\n",
    "import numpy as np\n",
    "outliers = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [\"int64\",\"float64\"]:\n",
    "        outliers.append(find_outliers(df[col]))\n",
    "        \n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check firt numeric columns by of outliers calling[0] . All values are saved as pandas.core.series\n",
    "# ilk sıradaki[0] nümerik değişken sütünu için kaydedilen outlier değerleri çektik. Değerler pandas.core.series. olarak kaydedilmiş\n",
    "outliers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we save all outlie values as a Pd.dataframe\n",
    "# Outlier değerleri Pandas dataframe olarak kaydedelim\n",
    "dff= pd.DataFrame(outliers)\n",
    "dff .head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52bf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values can be converted to a numpy array and saved as an array of unique values.\n",
    "# Bu değerleri numpy array olarak çevirip uniqe değerler olarak alalım ki tekrar etmesinler \n",
    "arr = dff.to_numpy()\n",
    "uniq =np.unique(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we save at as list and here we control the length of the list\n",
    "uniq =uniq.tolist()\n",
    "len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858617ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while creating the outliers list by implementing the IQR method, the \"0\" values have been accidently chosen as an outlier although its not an outlier\n",
    "# here we need to delete \"0\" from outliers list \n",
    "\n",
    "# outlier seçimi esnasında bir hata olarak \"0\" değerinin de outlier olarak kaydedildiğini gördük\n",
    "# bu hatadan kurtulmak için \"0\" değerini listeden çıkaracağız. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 in uniq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq.remove(0)\n",
    "0 in uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can replace all outlier values with np.nan\n",
    "#since it takes a lot time , I have already saved np.nan replaced dataframe before while i had been processing the first code line as below\n",
    "# now we can continue with the already processed sonout.csv\n",
    "\n",
    "# Aşağıdaki formül ile Features veri seti içindeki önceden tespit edilen yani outliers olan tüm değerleri sileceğiz ve np.nan yapacağız\n",
    "# süreç çok sürdüğü için bu işlemi önceden yapmış ve kaydetmiştik. Ara verdikten sonra kaldığımız yerden devam etmek için\n",
    "# önceden kaydettiğimiz sonout.csv üzerinden işlemlere devam ediyoruz.\n",
    "df = df.replace([uniq],np.nan)\n",
    "df.to_csv('sonout.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv (\"sonout.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641ded9",
   "metadata": {},
   "source": [
    "\n",
    "## Filling işlemleri\n",
    "### We could fill np.nan values with np.mean and np.median values calculated on daily basis. Since hourly recorded values differ a  lot during the day , it would ruin Central Tendency of whole dataframe. Thus, forwardfill and backfill methods have been implemented.\n",
    "\n",
    "\n",
    "## Filling işlemleri\n",
    "\n",
    "### Boşlukları günlük bazda hesaplanan mean ya da median değerler olarak doldurabilirdik . Ancak bu yöntem gün içi saatlik bazda veriler çok değişkenlik arzettiği için verinin merkezi eğiliminin bozulmasına neden oluyor. Bundan dolayı önce forwardfill sonra backfill yaptık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "names = df.columns[(df.dtypes.values == np.dtype('float64'))]\n",
    "type(names)\n",
    "\n",
    "names = names.unique()\n",
    "liste= list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in names:\n",
    "    df[col] = df[col].ffill()\n",
    "    \n",
    "for col in names:\n",
    "    df[col] = df[col].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if we still have null values\n",
    "# Eksik değerlerimiz kaldı mı diye kontrol ettik\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b960d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have imported power.csv again\n",
    "#Power.csv veri setini yendien çekelim\n",
    "power=pd.read_csv (\"power.csv\", sep=',')\n",
    "power.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b812fb",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "#### By using the exploratory data analysis, we can see the relationships between different variables. If a correlation coefficient is significant we can consider to use it for ML operations\n",
    "\n",
    "## Keşifçi Veri Analizi İşlemleri\n",
    "\n",
    "#### Koreleasyon Katsayılarını kullanarak değişkenler arasındaki ilişkiyi incelemek istiyoruz.Anlamlı ilişki bulunursa ML Modeli üzerinde kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the object type TimeStamp to date time variable\n",
    "# object olan TimeStamp'ı datetime olarak çevirelim.\n",
    "power['Timestamp'] =  pd.to_datetime(power['Timestamp'], infer_datetime_format=True)\n",
    "df['Timestamp'] =     pd.to_datetime(df['Timestamp'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets creat a train df,first we need to merge power to df\n",
    "# train etmek üzere yeni dataframe oluşturalım, önce merge işlemi yapmamız gerekiyor.Features ve power birleştirdik\n",
    "dftrain =pd.merge (df,power, on = \"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ef0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but first , it is better to see a correlation matrix on unmerged , df dataframe\n",
    "# ancak önce eski df veri seti üzerinden correlation matrix oluşturulalım,\n",
    "corr_matrix = df.corr().abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b92159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the varibles having very high correlation coefficient values may cause overfitting problem\n",
    "# so we need to have variables with cor values between  0.5 and 0.85 \n",
    "\n",
    "# Koreleasyon değerleri 0.85'den üstün olan değerleri almak yanıltıcı olur, overfitting sorunu ortaya çıkartabilir\n",
    "# bunun için en 0.5 ve 0.85 arasındaki koreleasyon matrislerini baz alacağız. \n",
    "\n",
    "# Here we extract uppertriangular matrix \n",
    "# Üst üçgen correlation matrix seçelim\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# here we remove less than 0.50 corr values\n",
    "#  0.50 üzeri corr matrix çıkaran değikenleri seçelim\n",
    "correlated = [column for column in upper.columns if any(upper[column] > 0.5)]\n",
    "\n",
    "#Here we remove more than 0.85 corr values\n",
    "# 0.85 üzeri corr matrix çıkaran değikenleri eleyelim\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae87ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we remove to_drop list we created as above from the df\n",
    "# elenenleri silelim\n",
    "df.drop (to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c584e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see best correla variables\n",
    "# en uygun corr matrix içeren değişkenleri aldık\n",
    "df.corr().unstack().drop_duplicates().sort_values().tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets merge it again after dropped variables\n",
    "# elenen değişkenlerden sonra yeniden  merge yaparak dataframe oluşturalım\n",
    "dftrain =pd.merge (df,power, on = \"Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793ef48",
   "metadata": {},
   "source": [
    "## Creating a Correalation Matrix of Independent Variables explaning the Power dependent variable the best \n",
    "\n",
    "## POWER hedef değişkenini en iyi açıklayan nümerik bağımsız değişkenleri listelemek için yeniden bir corr matrix alalım¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.corr().unstack().drop_duplicates().sort_values().tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, here we created a plot table of \"Converter_Control_Unit_Reactive_Power\"\n",
    "# Örnek olarak \"Converter_Control_Unit_Reactive_Power\" seçerek bir plot tablo aldık\n",
    "df.plot.line(y=\"Converter_Control_Unit_Reactive_Power\", use_index=True).figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in or do to understand it better here check descriptive statisctic of df train\n",
    "# yeni veri setimizin son durumuna bir bakmak üzere betimseli istatistik alalım\n",
    "dftrain.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9ae8b",
   "metadata": {},
   "source": [
    "##  Now we can split test and train datas\n",
    "\n",
    "## ML Modeli için test train seçimine başlayalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent and independent variables are chosen\n",
    "#  Bağımsız ve bağımlı değişken ayrımları yapıldı\n",
    "X = dftrain.iloc[:,1:-1].values\n",
    "y = dftrain.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  \"from sklearn.model_selection import TimeSeriesSplit\"\n",
    "\n",
    "## as the code written above is TimeSeriesSplit.But results are not good enough on kaggle\n",
    "### yukarıdaki gibi time series split de yapılabilirdi ancak denendi ve kaggle üzerinde fayda sağlamadı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d02a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we standartscale the inpdepentdent X variables \n",
    "# bağımsız X değişkenleri için standart scaling yapalım \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "X_train= scalerX.transform(X_train)\n",
    "X_test= scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c2d98",
   "metadata": {},
   "source": [
    "### Ont this dataset, predictions are made with LightGBM\n",
    "### Bu veri setinde LightGBM ile tahmin oluşturduk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightgbm\n",
    "import lightgbm as ltb\n",
    "\n",
    "model = ltb.LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748a70d",
   "metadata": {},
   "source": [
    "## As a result of predictions; test data achieved 0.93 rmse score on train data\n",
    "### I had also implemented XGBRegressor model and RandomizedSearchCV tuning however reuslts were not better on kaggle. It may work better with another dataset\n",
    "\n",
    "\n",
    "\n",
    "### Yukarıdaki train veri seti üzerinde test için ayrılan veri setimiz 0.93 score elde etti.\n",
    "###  Aslında önceden XGBRegressor modeli ve RandomizedSearchCV yöntemiye tuning yaparak tahminde bulunmuştuk ancak kaggle üzerinde score daha düşük geldi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a37d",
   "metadata": {},
   "source": [
    "# Now we create a modelling prediction for desired unknown future values. \n",
    "\n",
    "## Şimdi modelleme tahmini yapmak üzere  prediction veri seti oluşturalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we create the prediction dataset\n",
    "\n",
    "# prediction dataset oluşturalım.\n",
    "dfpred = pd.concat([df,power],axis =1)\n",
    "len(dfpred)\n",
    "dfpred=dfpred[[\"Gearbox_T1_High_Speed_Shaft_Temperature\", \"Tower_Acceleration_Normal\",\"Temperature_Trafo_3\", \"Temperature_Bearing_A\",\n",
    "\"Converter_Control_Unit_Reactive_Power\",\"Temperature_Shaft_Bearing_1\",\"Moment_D_Filtered\", \"Power(kW)\"  ]]\n",
    "dfpred = dfpred[dfpred[\"Power(kW)\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to choose datas for x_predict and run the model\n",
    "# X_predict üzerinde tahminde kullanılacak verileri seçtik ve modelimizi çalıştırdık.\n",
    "X_predict = dfpred.iloc[:,:-1].values\n",
    "\n",
    "\n",
    "predicted_y = model.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b140f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see predictions\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b21fd0",
   "metadata": {},
   "source": [
    "## Our predictions are ready, lets create the submission dataset\n",
    "\n",
    "### Yukarıda tahminlerimiz geldi, artık submission.csv veri setimizi oluşturabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "powerarray= pd.DataFrame(predicted_y, columns = ['Power(kW)'])\n",
    "powerarray.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample= pd.read_csv (\"sample_submission.csv\", sep=',')\n",
    "sample= pd.DataFrame().assign(Timestamp=sample['Timestamp'])\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesubmission =pd.merge (sample,powerarray, left_index=True,right_index=True)\n",
    "samplesubmission .head ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
